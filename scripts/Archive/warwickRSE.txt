Many research labs generate terrabyte sized imaging  datasets,  and this is particularly exacerbated for 3D imaging. Traditional analysis methods rely on manual labelling which requires significant training, time, and is expense. Particle detection and segmentation is the main  bottleneck in many image data analysis pipelines. When automatic analysis methods exist they usually suffer from low precision and/or recall. It is crucial to incorporate the  advancements  in  computer vision provided by machine learning to aid in the analysis of the vast data generated. Two models will be described in this poster. First a model for the semantic segmentation of arthritic zebrafish from micro-CT, with discussion of how this data facilitates the analysis of the development and aging in skeletal disease. Furthermore, a model was developed for the dense particle detection of 300nm glassy colloids. This model maintains the precision required for physics at 100% accross imaging parameters, and boosts recall from 40% to 75% . Forwarding the analysis of the generated data.